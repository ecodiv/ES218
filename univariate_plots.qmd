---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Visualizing univariate distributions


```{r echo=FALSE, message=FALSE}
source("libs/Common.R")
```


```{r echo = FALSE}
pkg_ver(c("dplyr", "ggplot2","tidyr"))
```


## Introduction

Let's create two batches of numbers, `a` and `b`. Even though the number of values are the same between both batches, this need not be the case in a univariate analysis.

```{r}
a  <- rep(12, 10)
b  <- rep(15, 10)
df <- data.frame(a, b)
```

How do the two batches differ?

```{r echo = FALSE, results = 'asis', fig.width = 2, fig.height=3}
#pander::pandoc.table(df, justify = 'left')
a.df <- gridExtra::tableGrob(data.frame(a), rows=NULL)
b.df <- gridExtra::tableGrob(data.frame(b), rows=NULL)
gridExtra::grid.arrange(a.df, b.df, nrow = 1, ncol = 2 ,newpage = FALSE)
```

If the difference wasn't obvious from the table view we can create a jittered point plot from the data. 

```{r fig.height=3, fig.width=3}
# To plot across categories, we need the data in a long (tidy) form
library(tidyr)
library(ggplot2)

dfl <- pivot_longer(df, names_to = "Category", values_to = "Value", 1:2)
ggplot(dfl,  aes(y = Value, x = Category, col = Category)) + 
             geom_jitter(position = position_jitter(height = 0, width=0.4))
```

It's clear that both batches differ by their uniform values, batch `a` is made up of the identical numbers, `12`, and batch `b` is made up of a different set of identical values, `15`. Note that because so many values overlapped, we made use of the `geom_jitter()` function which randomly *jitters* the data about their actual location.

Now let's compare a heterogeneous batch of numbers.

```{r}
# Generate some random numbers
set.seed(23)
a   <- round(runif(10, 5, 15))
b   <- round(runif(10, 10, 20))

# Create wide and long data frames
df  <- data.frame(a, b)
dfl <- pivot_longer(df, names_to = "Category", values_to = "Value", 1:2)

```


```{r echo = FALSE, results='asis',  fig.width = 2, fig.height=3}
a.df <- gridExtra::tableGrob(data.frame(a), rows=NULL)
b.df <- gridExtra::tableGrob(data.frame(b), rows=NULL)
gridExtra::grid.arrange(a.df, b.df, nrow = 1, ncol = 2 ,newpage = FALSE)
```


```{r fig.height=3, fig.width=4, echo=-1}
set.seed(14)
ggplot(dfl, aes(y = Value, x = Category, col = Category)) + 
            geom_jitter(position = position_jitter(height = 0, width = 0.2))
```

So how do these batches differ? This time, the difference is not so obvious. However, they *seem* to differ by their central value. For example, each batch's mean is:

```{r}
library(dplyr)
dfl %>% group_by(Category) %>% summarize(mean = mean(Value) )
```

The center value (aka **location**), is one summary statistic we can use to compare batches. Another property of a batch that we might also want to compare is its distribution (aka **spread**). For example, does the **spread**  between the two batches differ as well? It's difficult to tell from the above plot given that the batches are offset, so we'll **level** the batches by subtracting the means from their respective batches.

```{r fig.height=3, fig.width=4}
# Subtract the batch mean from each batch value
dfl2 <- dfl %>% 
  group_by(Category) %>%
  mutate(Spread = Value - mean(Value))

# Now plot the leveled batches
ggplot(dfl2, aes(y = Spread, x = Category, col = Category)) + 
             geom_jitter(position = position_jitter(height = 0, width=0.2))
```

Removing the *location* (or mean in our example) from each value facilitates our comparison of both spreads. From our working example we can, at best, say that the batches share the same range. But a spread can be characterized in many more ways than by its range. Next, we'll focus on four exploratory tools that will help us explore and quantify a dataset's spread. These are the **histogram**, the **boxplot**, the **density plot** and the **quantile plot**.

## Histograms

A histogram **bins** the values (usually in equal sized bins) and plots the frequency in which each *bin* is filled. For example, to create a histogram of batch `a` where each bin size covers **one** unit, we type:

```{r fig.height=2, fig.width=4.5}
ggplot(df, aes(x = a)) + geom_histogram(breaks = seq(6.5,16.5,by = 1), colour = "white")
```

Here, we are explicitly defining the bin width as 1 unit and the range as 6.5 to 16.5 via the parameter `breaks = seq(6.5,16.5,by=1)`. The `colour` parameter specifies the *outline* color. To change the fill color use the `fill` parameter instead. In our example, we have one value that falls in the first bin (bin ranging from 6.5 to 7.5), another value that falls in the second bin (bin value ranging from 7.5 to 8.5) and so on up to the second to last bin which has 3 values falling in it (bin covering the range 14.5 to 15.5). No values fall in the 15.5 to 16.5 bin range. 

We can modify the width of each bin. For example, to have each bin cover two units instead of one, type:

```{r fig.height=2, fig.width=4.5}
ggplot(df, aes(x = a)) + geom_histogram(breaks = seq(6.5,16.5,by = 2), colour = "white") 
```

You'll note that changing bin widths can alter the look of the histogram, this is particularly true when plotting large batches of values.

You can also opt to have the function determine the bin width by simply specifying the *number* of bins using the `bins =` parameter:

```{r fig.height=2, fig.width=4.5}
ggplot(df, aes(x = a)) + geom_histogram(bins = 12, colour = "white")
```

You'll note that the function will map the total number of observations falling in each bin to the y-axis. This may be easy to interpret, but it may make it difficult to compare batches of values having different number of observations. To demonstrate this, lets first create two batches of values of differing counts in a long form.

```{r}
set.seed(56) 
dfl3 <- data.frame(x = rnorm(200), 
                  grp = sample(c("a","b"), 200, replace = TRUE, prob = c(0.2,0.8)))
```

Now, lets generate a side-by-side histogram of the the batches:

```{r fig.width = 6, fig.height=3}
ggplot(dfl3, aes(x = x)) + geom_histogram(bins = 10) + facet_wrap(~ grp)
```

The differences in batch sizes results in differing bar heights. To resolve this issue, we can convert counts to density whereby the areas covered by each bin sum to `1`. Each bin's density value is computed from:

$$
density = \frac{count}{bin\ width * total\ observations}
$$
To see an example of this, let's revert back to the earlier dataset to facilitate a manual calculation of the density values. Here, we'll map the `..density..` aesthetic to the y-axis to have ggplot compute density values.

```{r fig.height=2, fig.width=4.5}
ggplot(df, aes(x = a)) + 
  geom_histogram(breaks = seq(5.8, 15.8, by = 2), colour = "white", aes(y = ..density..)) 
```

The first bin has one observation hence the density of observations within this bin interval is `1 / (2 * 10)` or `0.05`. Likewise, the second bin's density is computed as `2 / (2 * 10)` or `0.1`.

Mapping density instead of count to the y-axis thus generates this side-by-side plot from the two batch `dfl3` dataframe.

```{r fig.width = 6, fig.height=3}
ggplot(dfl3, aes(x = x, y=..density..)) + geom_histogram(bins = 10) + facet_wrap(~ grp)
```

This makes for a more appropriate comparison.

Note that you might come across an alternative to the `..density..` syntax in the R community that makes use of the `after_stat()` function. For example, the following code will generate the same density histogram.

```{r eval=FALSE}
ggplot(dfl3, aes(x = x, y = after_stat(density))) + geom_histogram(bins = 10) + facet_wrap(~ grp)
```

## Density plots

The histogram is not only sensitive to bin sizes, but it also suffers from discontinuities in its bins. In the following example, two histograms are generated using the same bin sizes and counts but with different starting x values. The orange marks along the x-axis show the location of the `a` values. The second histogram suggests a slightly bimodal (two peak) distribution while the one on the left suggests a unimodal distribution.

```{r fig.height=2, fig.width=8, echo=FALSE}
p1 <- ggplot(df, aes(x = a)) + geom_histogram(breaks = seq(6.5,16.5,by = 2), colour = "white") + geom_rug(col = "darkorange", cex = 2, alpha = 0.5) +
  ggtitle("Bin starts at 6.5")
p2 <- ggplot(df, aes(x = a)) + geom_histogram(breaks = seq(5.8,15.8,by=2), colour = "white") + geom_rug(col = "darkorange", cex = 2, alpha = 0.5) +
  ggtitle("Bin starts at 5.8")
gridExtra::grid.arrange(p1, p2, nrow = 1)
```

One workaround to the histogram limitations is to compute density values on *overlapping* bins. For example, let's take the first bin and have it count the number of values between 5 and 9 (exclusive), then divide that number by the total number of values times the bin width--this gives us two observations falling in the bin thus a density value of `2 / (10 * 4) = 0.05`. The following plot shows the bin as well as an orange dot representing the bin's midpoint.

```{r fig.height=2, fig.width=4.5, echo=FALSE}
bw <- 4
xi <- seq(min(a)-bw/2, max(a) - bw/2, by=1)

dens <- vector(length=length(xi))
for (i in seq_along(xi)) {
 dens[i] <- sum(( a >= xi[i] ) & (a < xi[i] + bw) )/ (bw * length(a))
}

dens.bins <- data.frame(xmin = xi,
                  xmax = xi + bw,
                  ymin = 0,
                  ymax=dens)

ggplot() +
  geom_rect( dens.bins[1,], mapping=aes(xmin=xmin,xmax=xmax,ymin=ymin,ymax=ymax),alpha=0.3) + 
  geom_rug( data.frame(a), mapping=aes(x=a),col="darkred", cex=2, alpha=0.5) +
  ylim(range(dens.bins[, 3:4])) +
  geom_segment(data=dens.bins[1,], aes(x=(xmin + xmax)/2, xend =(xmin + xmax)/2,
                                  y=0, yend=ymax), lty=2, alpha=0.6) +
  geom_point(dens.bins[1,], mapping=aes(x=(xmin + xmax)/2, y=ymax), col="orange", cex=2.5) +
  scale_x_continuous(lim = range(dens.bins[, 1:2]), breaks = 5:18)

```

Next, we shift the bin over by one `x` unit, then we calculate the density of observations in the same way it was computed for the first bin. The density value is plotted as an orange dot. Note how the bin overlaps partially with the first bin.

```{r fig.height=2, fig.width=4.5, echo=FALSE}
ggplot() +
  geom_rect( dens.bins[1,], mapping=aes(xmin=xmin,xmax=xmax,ymin=ymin,ymax=ymax),alpha=0.1,col="white") +
  geom_rect( dens.bins[2,], mapping=aes(xmin=xmin,xmax=xmax,ymin=ymin,ymax=ymax),alpha=0.3) +
  geom_rug( data.frame(a), mapping=aes(x=a),col="darkred", cex=2, alpha=0.5) +
  ylim(range(dens.bins[, 3:4])) +
  geom_segment(data=dens.bins[2,], aes(x=(xmin + xmax)/2, xend =(xmin + xmax)/2,
                                  y=0, yend=ymax), lty=2, alpha=0.6) +
  geom_point(dens.bins[1:2,], mapping=aes(x=(xmin + xmax)/2, y=ymax), col="orange", cex=2.5) +
  scale_x_continuous(lim = range(dens.bins[, 1:2]), breaks = 5:18)

```

The same process is repeated for the third bin.

```{r fig.height=2, fig.width=4.5, echo=FALSE}
ggplot() +
  geom_rect( dens.bins[1:2,], mapping=aes(xmin=xmin,xmax=xmax,ymin=ymin,ymax=ymax),alpha=0.1,col="white") +
  geom_rect( dens.bins[3,], mapping=aes(xmin=xmin,xmax=xmax,ymin=ymin,ymax=ymax),alpha=0.3) + 
  geom_rug( data.frame(a), mapping=aes(x=a),col="darkred", cex=2, alpha=0.5) +
  ylim(range(dens.bins[, 3:4])) +
  geom_segment(data=dens.bins[3,], aes(x=(xmin + xmax)/2, xend =(xmin + xmax)/2,
                                  y=0, yend=ymax), lty=2, alpha=0.6) +
  geom_point(dens.bins[1:3,], mapping=aes(x=(xmin + xmax)/2, y=ymax), col="orange", cex=2.5) +
  scale_x_continuous(lim = range(dens.bins[, 1:2]), breaks = 5:18)

```

The process is repeated for each bin until the last bin is reached. (Note that some of the `a` values are duplicates such as `13` and `15`--hence the high density values for the upper range).

```{r fig.height=2, fig.width=4.5, echo=FALSE}
ggplot() +
  geom_rect( dens.bins[1:8,], mapping=aes(xmin=xmin,xmax=xmax,ymin=ymin,ymax=ymax),alpha=0.1,col="white") +
  geom_rect( dens.bins[9,], mapping=aes(xmin=xmin,xmax=xmax,ymin=ymin,ymax=ymax),alpha=0.5) + 
  geom_rug( data.frame(a), mapping=aes(x=a),col="darkred", cex=2, alpha=0.5) +
  ylim(range(dens.bins[, 3:4])) +
  geom_segment(data=dens.bins[9,], aes(x=(xmin + xmax)/2, xend =(xmin + xmax)/2,
                                  y=0, yend=ymax), lty=2, alpha=0.6) +
  geom_point(dens.bins[1:9,], mapping=aes(x=(xmin + xmax)/2, y=ymax), col="orange", cex=2.5) +
  scale_x_continuous(lim = range(dens.bins[, 1:2]), breaks = 5:18)

```

If we remove the bins and connect the dots, we end up with a **density trace**. 

```{r fig.height=2, fig.width=4.5, echo=FALSE}
ggplot() +
  geom_rug( data.frame(a), mapping=aes(x=a),col="darkred", cex=2, alpha=0.5) +
  ylim(range(dens.bins[, 3:4])) +
    geom_line(dens.bins[1:9,], mapping=aes(x=(xmin + xmax)/2, y=ymax), col="black") +
  geom_point(dens.bins[1:9,], mapping=aes(x=(xmin + xmax)/2, y=ymax), col="orange", cex=2.5) +
  scale_x_continuous(lim = range(dens.bins[, 1:2]), breaks = 5:18)

```

A property associated with the density trace is that the area under the curve sums to one since each density value represents the *local* density at `x`.

So far, when we computed the density values, we assigned an equal weight to each point within their assigned bin--regardless how far the values were to the bin's midpoint. This resulted in some *raggedness* to the plot. To smooth out the plot, we can apply different weights to each point such that points closest to the bin's midpoint are assigned greater weight than the ones furthest from the midpoint. A Gaussian function can be used to generate the weights. The following figure depicts the difference in weights assigned to any point falling within the first bin whose range covers the interval 4 to 8 centered on 6.

```{r fig.height=2, fig.width=4.5, echo = FALSE}

d.rec <- density(6, from = 4, to = 8, width = 4, kernel = "rectangular")
p1 <- ggplot() +  geom_area(data = data.frame(x= d.rec$x, y= d.rec$y), mapping = aes(x,y), fill="grey") +
  geom_rug( data.frame(x=6), mapping=aes(x=x), col = "darkred", cex=2, alpha=0.5) + 
  ylim(0,.65) + ylab("weight")

d.gaus <- density(6, from = 4, to = 8, width = 2.5, kernel = "gaussian")
p2 <- ggplot() +  geom_area(data = data.frame(x= d.gaus$x, y= d.gaus$y), mapping = aes(x,y), fill="grey") +
  geom_rug( data.frame(x=6), mapping=aes(x=x), col = "darkred", cex=2, alpha=0.5) + 
  ylim(0,.65) + ylab(NULL)


gridExtra::grid.arrange(p1, p2, nrow = 1)

```

With the rectangular weight, all points within a bin width are assigned equal weight. With the Gaussian weight, points closest to the bin center are assigned greater weight than those furthest from the center. Ggplot's density function defaults to the Gaussian weighting strategy. The  weight type is referred to as a **kernel**.

You can generate a density plot of the data using the `geom_density` function.

```{r fig.height=2, fig.width=4.5}
ggplot(df, aes(x = a)) + geom_density(fill = "grey60")
```

The function adopts the `gaussian` weight function and will automatically define the bandwidth (analogous in concept to the bin width). TO adopt a rectangualr weight, set the `kernel` parameter to `"rectangular"`.

```{r fig.height=2, fig.width=4.5}
ggplot(df, aes(x = a)) + geom_density(fill = "grey60", kernel = "rectangular")
```

Note the raggedness in the plot. In this course, we will default to the Gaussian kernel.

You can modify the *smoothness* of the density plot by adjusting its bandwidth argument `bw`. Here, the bandwidth defines the standard deviation of the Gaussian function.

```{r fig.height=2, fig.width=4.5}
ggplot(df, aes(x = a)) + geom_density(fill = "grey60", bw = 1)
```

## Boxplots

A boxplot is another popular plot used to explore distributions. In `ggplot2` we use the `geom_boxplot()` function as in,

```{r fig.height=1.2, fig.width=6.5}
ggplot(df, aes(x = a)) + geom_boxplot() + 
          xlab(NULL) + theme(axis.text.y = element_blank(), 
                             axis.ticks.y = element_blank()) 
```

The `geom_boxplot` function can map the values to the x-axis or to the y-axis. Traditionally, it's mapped to the y-axis. Here, we choose to map it the x-axis. You'll note the extra functions `xlab(NULL) + theme(axis.text.y=element_blank(), ... )`; these are added to suppress labels and values/tics along the y-axis given that their default values do not serve a purpose here.

The following figure describes the anatomy of a boxplot.


![](./img/Boxplot.png)

The boxplot provides us with many meaningful pieces of information. For example, it gives us a center value: the median. It also tells us where the middle 50% of the values lie along the full range of values (in our example, approximately 50% of the values lie between 9.5 and 14.5). This range is referred to as the **interquartile** range (or **IQR** for short). Note that this is only an approximation given that some datasets may not lend themselves well to defining exactly 50% of their central values. For example, our batch only has four data points falling within the interquartile range because of tied values.

The long narrow lines extending beyond the interquartile range are referred to as the **adjacent values** or as **whiskers**. They represent either 1.5 times the width between the median and the nearest interquartile value *or* the most extreme value, whichever comes first.

Sometimes, you will encounter values that fall outside of the lower and/or upper adjacent values; such values are often referred to as **outliers**.

### Not all boxplots are created equal!

Not all boxplots are created equal. There are many different ways in which quantiles can be defined. For example, some will compute a quantile as $( i - 0.5) / n$ where $i$ is the n^th^ element of the batch of data and $n$ is the total number of elements in that batch. This is the method implemented by Bill Cleveland and we will refer to this method as *Cleveland's quantile method*. This also happens to be the method implemented by the base's `boxplot` function; this explains its different boxplot output compared to `geom_boxplot` in our working example:

```{r, fig.height=1.4, fig.width=6.5, echo=2}
OP <- par(mar=c(2,1,1,1), bty="n")
boxplot(a, horizontal = TRUE)
grid(lty = 1, col = "grey90")
boxplot(a, horizontal = TRUE, add = TRUE)
par(OP)
```

The upper and lower quartiles differ from those of `ggplot` since the three `15` values (these are the maximum values in batch `a`) end up falling inside the interquartile range following the aforementioned quantile definition. This eliminates any upper whiskers.  In most cases, however, the difference will not matter as long as you *adopt the same boxplot procedure when comparing batches*.

### Implementing different quantile types in `geom_boxplot`

If you wish to implement different $f$-value calculations offered by the ` quantile` function, you will need to create a custom function as follows:

```{r fig.height=1.2, fig.width=6.5}
# Function to extract quantiles given an f-value type
qtl.bxp <- function(x, type = 5) {
  qtl <- quantile(x, type = type)
   df <- data.frame(ymin  = qtl[1], ymax = qtl[5], 
                    upper = qtl[4], lower = qtl[2], middle = qtl[3])
}

# Plot the boxplot
ggplot(df, aes(x = "", y = a)) + 
  stat_summary(fun.data = qtl.bxp, fun.args = list(type = 5),
               geom = 'boxplot') +
  xlab(NULL) + theme(axis.text.y = element_blank()) +
  coord_flip()

```

Note the use of `stat_summary` instead of `geom_boxplot`. The `type` argument is the quantile type implemented in the `quantile` function. 

## Quantile plots

A quantile plot generates a point plot that joins the quantile to each value in a batch. The boxplot is a special case of the $f$-quantile function in that it only returns the 1^st^, 2^nd^ (median) and 3^rd^ quartiles. The $f$-quantile returns the $full$ range of quantile values. The quantile is directly related to the concept of a percentile: it identifies the fraction of the batch of numbers that is less than a value of interest.  The following figure describes the anatomy of a quantile plot. 

![](./img/Quantile.png)

The x-axis shows the $f$-values: the full range of fractions. The y-axis is the $f$-quantile, $q(f)$, which shows the sorted batch values (from smallest to largest). The points in the plot link the values on the y-axis to the $f$-values on the x-axis. For example, the $f$-value of 0.25 (~the 25^th^ percentile) is associated with the $q(f)$ value of 9 meaning that 25% of the values in the dataset have values of 9 or less. Likewise, the $f$-value of 0.5 (the median) is associated with a $q(f)$ value of 12.5 implying that half of the dataset's values are 12.5 or less. The boxplot is shown alongside the quantile plot to highlight the analogy.

### Computing the $f$-quantile

 Computing $f$ requires that the batch of numbers be ordered from smallest to largest.

```{r}
a.o <- sort(a)
a.o
```

The concept of sorting values may seem benign, but it is fundamental to many  EDA procedures that require robust techniques.

With the numbers sorted, we can proceed with the computation of $f$ following *Cleveland's* method:


$$
f_i = \frac{i - 0.5}{n} 
$$

where $i$ is the n^th^ element of the batch of data and $n$ is the total number of elements in that batch. As noted in the Boxplots section, there are many ways one can compute a quantile, however, the differences may not matter much.

For each value in `a`, the $f$ value is thus:

```{r}
i     <- 1 : length(a)
f.val <- (i - 0.5) / length(a)  # Compute the f-value
a.fi  <- data.frame(a.o, f.val)
```

Note that in the last line of code, we are appending the ordered representation of `a` to `f.val` given that `f.val` assumes an ordered dataset. The data frame `a.fi` should look like this:

```{r echo = FALSE, results='asis'}
pander::pandoc.table(a.fi, justify = 'left')
```

It may be desirable at times to find a value associated with a quantile that might not necessarily match an exact value in our batch. For example, there is no value in `a` associated with a quantile of $0.5$; this is because we have an even number of values in our dataset. The solution is to interpolate a value based on a desired quantile. The `quantile()` function does just that. For example, to find the value associated with a quantile of $0.5$, type:

```{r}
quantile(a, 0.5)
```

If we want to get quantile values for multiple fractions, simply wrap the fractions with the `c()` function:

```{r}
quantile(a, c(0.25, 0.5, 0.75))
```

The `quantile` function is designed to accept different quantile algorithms. To see the list of algorithm options, type `?quantile` at a command prompt. By default, R adopts algorithm `type = 7`. To adopt Cleveland's algorithm, set `type = 5`. E.g.:

```{r}
quantile(a, c(0.25, 0.5, 0.75), type = 5)
```

Note the difference in the upper quartile value compared to the default `type = 7`.

### Creating a quantile plot

A batch's quantile is best viewed as a plot where we plot the batch values as a function of the $f$-values:

```{r fig.height=3, fig.width=3}
ggplot(a.fi, aes(x = f.val, y = a.o)) + geom_point() + xlab("f-value")
```


#### Using ggplot's `qq` geom

If you did not want to go through the trouble of computing the $f$-values and the dataframe `a.fi`, you could simply call the function `stat_qq()` as in:

```{r fig.height=3, fig.width=3}
ggplot(df, aes(sample = a)) + stat_qq(distribution = qunif) + xlab("f-value")
```

However, ggplot's `stat_qq` function does not adopt Cleveland's $f$-value calculation. Hence, you'll notice a slight offset in position along the x-axis. For example, the third-to-last point has an $f$-value of `0.744` instead of an $f$-value of `0.75` as calculated using Cleveland's method.

Also note the change in mapping parameter: `sample = a`. We are no longer specifying the axis to map to. Instead, we are passing the **sample** to the `stat_qq` function which then chooses to map the sorted `a` values to the y-axis.

## How quantile plots behave in the face of skewed data

It can be helpful to simulate distributions of difference skewness to see how a quantile plot may behave. In the following figure, the top row shows the different density distribution plots and the bottom row shows the **quantile plots** for each distribution (note that the x-axis maps the f-values).


```{r, echo=FALSE, fig.width=9,fig.height=4}
# q.q function
# =============
# Function will generate q-q  plot and line
# given two vectors: a (y-axis) and b (x-axis)

q.q <- function(a,b, lin=TRUE){
  probs <-  c(0.25, 0.75)
  la <- length(a)
  lb <- length(b)
  a  <- sort(a)
  b  <- sort(b)
  fa <- ( 1:la - 0.5) / la
  fb <- ( 1:lb - 0.5) / lb
  if (la < lb) {
    b <- approx(fb, b, fa)$y 
  } else if( la > lb) {
    a <- approx(fa, a, fb)$y
  } else{}
  y <- quantile(a,probs)
  x <- quantile(b,probs)
  slope <- diff(y)/diff(x)
  int <- y[1] - slope * x[1]
  plot(a ~ b, cex=0.7, pch=20, col="blue")
  if (lin == TRUE) {
    abline(int,slope)
  }
}

#################
# Quantile plots
#################

# Set sample size and compute uniform
# values

n  <- 1000 # Number of simulated samples
fi <- (1:n - 0.5)/n
b.shp1 <- c(1, 5 , 50, 10, 10)
b.shp2 <- c(10,  10 , 70, 5, 1)

# Generate quantile plots (uniform q-q)

b  <- fi

OP <- par( mfcol = c(2,5), mar = c(2,2,1,1) )
for (i in 1:5 ) {
  a <- qbeta(fi,shape1=b.shp1[i], shape2 = b.shp2[i])
  plot(density(a),main=NA,xlab=NA)
  q.q(a,b, lin=FALSE)
}
par(OP)
```

